{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d060a5",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44aa5e",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Mariam Bachar (A16217374)\n",
    "- Alexandra Hernandez (A16730685)\n",
    "- Brian Kwon (A16306826)\n",
    "- Andrew Uhm (A16729684)\n",
    "- Ethan Wang (A17229824)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270600d1",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e792a5",
   "metadata": {},
   "source": [
    "*Do certain keywords as identified by CLIP correlate with the popularity (as measured by the equivalent of “likes”) that artwork receives on social media?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19bb8f",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae5873",
   "metadata": {},
   "source": [
    "We predict that digital artwork that contains certain keywords as predicted by CLIP (painting vs. watercolor vs. digital) will indeed have a positive correlation to popularity on social media. As humans observing what is popular, we notice that certain features tend to repeat themselves across posts, which leads us to believe a correlation will be found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c1366",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edecc4",
   "metadata": {},
   "source": [
    "Our ideal dataset would be a representative sample of images representing most genres of art. Our variables would be the image, the caption of an image, and their associated ‘likes’. We would want a decent amount of observations spanning that representative sample aforementioned, somewhere in the ballpark of ~3000 images alongside their “like” count and the original artist’s follow count. Files can be anonymized with integer IDs. From there, we would process the images to extract the captions using CLIP and store that to the corresponding data point’s image as well. Ideally images would all be the same size. Furthermore the ideal dataset would have published dates as well in order to make comparisons to past trends. In order to define popularity, we would define it as a number of likes in proportion to the maximum number of likes in our dataset, defining it regressively instead of binarily.\n",
    "\n",
    "A real data set we could use could be from DeviantArt’s API. We acknowledge that this data is different from our ideal. For one, the images are not perfectly square. We will thus crop and size the images down to a predetermined size (e.g. 768x768) in order to normalize. DeviantArt also likely has its own culture, which means our findings may not be representative of other social media and by taking images from their home page we may not be seeing old posts. Furthermore the fields that we require are actually optional, so we would have to filter to images that actually have all the data fields we require filled out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47d306",
   "metadata": {},
   "source": [
    "# Ethics and Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db5e42",
   "metadata": {},
   "source": [
    "There are a number of ethical concerns regarding this research question that we must be mindful of as we analyze data. The most obvious issue is that we are tagging artwork as unpopular by virtue of not identifying said artwork as popular. However, this should not be a strong issue as we are not presenting identifying pieces of information of specific pieces of artwork or individual artists, so it should not be possible to label a specific artwork or artist as “unpopular”.\n",
    "\n",
    "In terms of normalization, a possible solution would be to take a ratio between the number of likes on the artwork and the number of followers that certain artist has in order to take into account the disparity between larger artists and smaller artists in terms of popularity, as more popular artists would get more likes due to a larger audience. Additionally, it is entirely possible that our analysis may exclude cultural influences of minority groups. Since those residing in developed countries have more leisure time/resources (such as drawing software or drawing e-tablets), it is plausible that most digital art posted to social media is likely from developed countries. Thus, the work we analyze may disproportionately represent artwork and cultural trends of majority groups of developed countries while glossing over minority groups, which tend to be similar across developed countries.\n",
    "\n",
    "Finally, because the artworks are on a public forum, they have consented to allowing their art to be analyzed. The Deviantart TOS states that you cannot “reproduce, distribute, publicly display or perform, or prepare derivative works”, which does not include the use of the artworks for an analytic survey. Although there is no clear-cut solution for this, it serves us well to keep this fact in mind when drawing conclusions upon our analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f04152",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e40258",
   "metadata": {},
   "source": [
    "- Dataset Name: deviation_info\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/blob/master/deviation_info.csv\n",
    "- Number of observations: 1188\n",
    "\n",
    "This dataset is a set of deviations (that is images from deviantart) that contain deviation ids and metadata about the deviation itself as well as the author. It does not include the actual images.\n",
    "\n",
    "- Dataset Name: caption_info\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/blob/master/caption_info.csv\n",
    "- Number of observations: 1188\n",
    "\n",
    "This dataset is a set of captions processed from the image, corresponding to a deviation id. It was processed using the CLIP interrogator in Automatic1111's stable diffusion webui.\n",
    "\n",
    "- Dataset Name: images\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/tree/master/images\n",
    "- Number of observations: 1198\n",
    "\n",
    "This dataset is a directory of images in png format that are named based on their corresponding deviation ids, it is the actual images. There are 10 extra images in here that aren't found in our other datasets.\n",
    "\n",
    "\n",
    "All of the datasets were built from scraping, and use deviation ids as their identifiers. Because of this, we can easily add them together based on those deviation ids if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa97586",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import deviantart\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# DeviantArt API: https://www.deviantart.com/developers/http/v1/20210526\n",
    "# Open-Source Python wrapper for DA API: https://github.com/neighbordog/deviantart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc52e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a pd df from the csv file if it exists, else creates a blank df\n",
    "csv_file = 'deviation_info.csv'\n",
    "try:\n",
    "    deviation_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    deviation_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate API keys in case of requesting issues\n",
    "andrew_DA_API = deviantart.Api(\"25542\", \"61a232f232df245f2560a3cb72ecc535\")\n",
    "ethan_DA_API = deviantart.Api(\"25492\", \"06217cf59e73b401dc0a14d00857a793\")\n",
    "\n",
    "# access token is da.access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3761b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README: use your own token\n",
    "cur_access = andrew_DA_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a36257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images we want to fetch * 10\n",
    "n = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    print('on iteration', i, '* 10')\n",
    "    # grab 10 images at a time. DeviantArt calls their posts \"deviations\".\n",
    "    # TODO: consider timerange 'onemonth'\n",
    "    deviations = cur_access.browse(endpoint='popular', timerange='alltime', offset=i*10, limit=10)['results']\n",
    "    \n",
    "    for deviation in deviations:\n",
    "        # saves image to file by deviation id using url for local CLIP analysis\n",
    "        if deviation.content is None:\n",
    "            print('null deviation on iteration', i)\n",
    "            continue\n",
    "        url = deviation.content['src']\n",
    "        dId = deviation.deviationid\n",
    "        filename = f\"images/{dId}.png\"\n",
    "        path = Path(filename)\n",
    "        if path.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            open(filename, 'w').close()\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "        # these serve as examples of how to make a request when the python wrapper doesn't work\n",
    "        username = deviation.author.username\n",
    "        request = f\"https://www.deviantart.com/api/v1/oauth2/user/profile/{username}?access_token={cur_access.access_token}&expand=user.stats\"\n",
    "        response = requests.get(request)\n",
    "        authorData = response.json()\n",
    "        authorWatchers = authorData['user']['stats']['watchers']\n",
    "        authorPageViews = authorData['stats']['profile_pageviews'] # deemed unnecessary?\n",
    "        authorDeviations = authorData['stats']['user_deviations']\n",
    "        \n",
    "        request = f\"https://www.deviantart.com/api/v1/oauth2/deviation/metadata?access_token={cur_access.access_token}&deviationids={deviation}&ext_stats=True\"\n",
    "        response = requests.get(request)\n",
    "        metaData = response.json()\n",
    "        views = metaData['metadata'][0]['stats']['views']\n",
    "        \n",
    "        # gathering relevant data, turning it into a new observation\n",
    "        row = {\n",
    "            'Deviation ID': deviation.deviationid,\n",
    "            'Title': deviation.title,\n",
    "            'Author': deviation.author,\n",
    "            'Views': views,\n",
    "            'Favorites': deviation.stats['favourites'],\n",
    "            'Comments': deviation.stats['comments'],\n",
    "            'URL Link': deviation.url,\n",
    "            'Date Posted': datetime.fromtimestamp(int(deviation.published_time)),\n",
    "            'Height': deviation.content['height'],\n",
    "            'Width': deviation.content['width'],\n",
    "            'File Size': deviation.content['filesize'],\n",
    "            'Author Watchers': authorWatchers,\n",
    "            'Author Page Views': authorPageViews,\n",
    "            'Author Deviations': authorDeviations\n",
    "        }\n",
    "        row_df = pd.DataFrame(row, index=[0])\n",
    "        deviation_df = pd.concat([deviation_df, row_df], ignore_index=True)\n",
    "        \n",
    "    # when running on the most popular posts, we will likely get duplicates. remove them.\n",
    "    deviation_df = deviation_df.drop_duplicates(subset='Deviation ID')\n",
    "    \n",
    "    # grab every 15 seconds in order to adhere to DeviantArt fetch rate.\n",
    "    if n > 1:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc960b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put our df into a csv file so scraping can be collaborative\n",
    "# to_csv overwrites but should be ok since we are reading from the csv to populate the df anyways\n",
    "deviation_df.to_csv('deviation_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14867766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deviation ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Views</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Comments</th>\n",
       "      <th>URL Link</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>File Size</th>\n",
       "      <th>Author Watchers</th>\n",
       "      <th>Author Page Views</th>\n",
       "      <th>Author Deviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6797CD44-47EA-B405-9377-5D41D83F33FE</td>\n",
       "      <td>A l'aise...</td>\n",
       "      <td>thrumyeye</td>\n",
       "      <td>2022815</td>\n",
       "      <td>31101</td>\n",
       "      <td>2385</td>\n",
       "      <td>https://www.deviantart.com/thrumyeye/art/A-l-a...</td>\n",
       "      <td>2011-02-17 23:43:04</td>\n",
       "      <td>599</td>\n",
       "      <td>900</td>\n",
       "      <td>408379</td>\n",
       "      <td>36527</td>\n",
       "      <td>1354598</td>\n",
       "      <td>2179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83750DFB-D23E-00A3-DF4F-D164A07DF605</td>\n",
       "      <td>Tiger cub</td>\n",
       "      <td>Kamirah</td>\n",
       "      <td>1271452</td>\n",
       "      <td>20720</td>\n",
       "      <td>2097</td>\n",
       "      <td>https://www.deviantart.com/kamirah/art/Tiger-c...</td>\n",
       "      <td>2008-07-11 06:10:53</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "      <td>523370</td>\n",
       "      <td>45123</td>\n",
       "      <td>7500677</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8F1ED8A1-88A5-861A-F83B-77916A1481A0</td>\n",
       "      <td>Baby Steps 0268P</td>\n",
       "      <td>Sooper-Deviant</td>\n",
       "      <td>1097719</td>\n",
       "      <td>24050</td>\n",
       "      <td>937</td>\n",
       "      <td>https://www.deviantart.com/sooper-deviant/art/...</td>\n",
       "      <td>2009-10-05 06:55:37</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>4493241</td>\n",
       "      <td>35586</td>\n",
       "      <td>1438276</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66D5BA39-C0D4-7A95-52FF-C0694149142E</td>\n",
       "      <td>Sky Turtle</td>\n",
       "      <td>yuumei</td>\n",
       "      <td>1056323</td>\n",
       "      <td>23103</td>\n",
       "      <td>918</td>\n",
       "      <td>https://www.deviantart.com/yuumei/art/Sky-Turt...</td>\n",
       "      <td>2014-01-23 10:46:57</td>\n",
       "      <td>532</td>\n",
       "      <td>1000</td>\n",
       "      <td>390864</td>\n",
       "      <td>397714</td>\n",
       "      <td>22504533</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1C127EBF-EFF7-7BC4-004D-0355A2856C05</td>\n",
       "      <td>Riders</td>\n",
       "      <td>sandara</td>\n",
       "      <td>1240225</td>\n",
       "      <td>15643</td>\n",
       "      <td>512</td>\n",
       "      <td>https://www.deviantart.com/sandara/art/Riders-...</td>\n",
       "      <td>2013-09-30 23:37:02</td>\n",
       "      <td>770</td>\n",
       "      <td>1200</td>\n",
       "      <td>832434</td>\n",
       "      <td>212588</td>\n",
       "      <td>7566524</td>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>11FDBF2D-B961-3098-11A0-4C152C805497</td>\n",
       "      <td>Treasury</td>\n",
       "      <td>meganjoy</td>\n",
       "      <td>49453</td>\n",
       "      <td>5245</td>\n",
       "      <td>320</td>\n",
       "      <td>https://www.deviantart.com/meganjoy/art/Treasu...</td>\n",
       "      <td>2012-04-02 04:34:29</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>440725</td>\n",
       "      <td>38660</td>\n",
       "      <td>1861256</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>F2D8D2C1-214D-DF09-EB82-70559AC93FAF</td>\n",
       "      <td>Apotlas</td>\n",
       "      <td>artozi</td>\n",
       "      <td>171100</td>\n",
       "      <td>14474</td>\n",
       "      <td>717</td>\n",
       "      <td>https://www.deviantart.com/artozi/art/Apotlas-...</td>\n",
       "      <td>2014-10-09 04:35:37</td>\n",
       "      <td>1200</td>\n",
       "      <td>900</td>\n",
       "      <td>290057</td>\n",
       "      <td>11424</td>\n",
       "      <td>281437</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>C82D8DF4-BF3B-88AA-2B42-F40A7872C00F</td>\n",
       "      <td>Spirited Away</td>\n",
       "      <td>nuriko-kun</td>\n",
       "      <td>198833</td>\n",
       "      <td>23043</td>\n",
       "      <td>1467</td>\n",
       "      <td>https://www.deviantart.com/nuriko-kun/art/Spir...</td>\n",
       "      <td>2011-04-01 10:44:45</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>545063</td>\n",
       "      <td>57989</td>\n",
       "      <td>1775664</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>0F1A7C6C-3159-FE8D-208C-9EC3BDCE6073</td>\n",
       "      <td>Stamp: I'm not weird</td>\n",
       "      <td>Roxy317</td>\n",
       "      <td>80946</td>\n",
       "      <td>14291</td>\n",
       "      <td>1351</td>\n",
       "      <td>https://www.deviantart.com/roxy317/art/Stamp-I...</td>\n",
       "      <td>2007-11-11 05:17:17</td>\n",
       "      <td>56</td>\n",
       "      <td>99</td>\n",
       "      <td>18015</td>\n",
       "      <td>227</td>\n",
       "      <td>73692</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>EEA099A7-1861-4184-47A8-5C5F41B2C404</td>\n",
       "      <td>A Moment Alone</td>\n",
       "      <td>yuumei</td>\n",
       "      <td>88231</td>\n",
       "      <td>9766</td>\n",
       "      <td>196</td>\n",
       "      <td>https://www.deviantart.com/yuumei/art/A-Moment...</td>\n",
       "      <td>2017-06-25 12:01:18</td>\n",
       "      <td>1326</td>\n",
       "      <td>1500</td>\n",
       "      <td>1225292</td>\n",
       "      <td>397713</td>\n",
       "      <td>22504608</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1188 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Deviation ID                 Title  \\\n",
       "0     6797CD44-47EA-B405-9377-5D41D83F33FE           A l'aise...   \n",
       "1     83750DFB-D23E-00A3-DF4F-D164A07DF605             Tiger cub   \n",
       "2     8F1ED8A1-88A5-861A-F83B-77916A1481A0      Baby Steps 0268P   \n",
       "3     66D5BA39-C0D4-7A95-52FF-C0694149142E            Sky Turtle   \n",
       "4     1C127EBF-EFF7-7BC4-004D-0355A2856C05                Riders   \n",
       "...                                    ...                   ...   \n",
       "1183  11FDBF2D-B961-3098-11A0-4C152C805497              Treasury   \n",
       "1184  F2D8D2C1-214D-DF09-EB82-70559AC93FAF               Apotlas   \n",
       "1185  C82D8DF4-BF3B-88AA-2B42-F40A7872C00F         Spirited Away   \n",
       "1186  0F1A7C6C-3159-FE8D-208C-9EC3BDCE6073  Stamp: I'm not weird   \n",
       "1187  EEA099A7-1861-4184-47A8-5C5F41B2C404        A Moment Alone   \n",
       "\n",
       "              Author    Views  Favorites  Comments  \\\n",
       "0          thrumyeye  2022815      31101      2385   \n",
       "1            Kamirah  1271452      20720      2097   \n",
       "2     Sooper-Deviant  1097719      24050       937   \n",
       "3             yuumei  1056323      23103       918   \n",
       "4            sandara  1240225      15643       512   \n",
       "...              ...      ...        ...       ...   \n",
       "1183        meganjoy    49453       5245       320   \n",
       "1184          artozi   171100      14474       717   \n",
       "1185      nuriko-kun   198833      23043      1467   \n",
       "1186         Roxy317    80946      14291      1351   \n",
       "1187          yuumei    88231       9766       196   \n",
       "\n",
       "                                               URL Link          Date Posted  \\\n",
       "0     https://www.deviantart.com/thrumyeye/art/A-l-a...  2011-02-17 23:43:04   \n",
       "1     https://www.deviantart.com/kamirah/art/Tiger-c...  2008-07-11 06:10:53   \n",
       "2     https://www.deviantart.com/sooper-deviant/art/...  2009-10-05 06:55:37   \n",
       "3     https://www.deviantart.com/yuumei/art/Sky-Turt...  2014-01-23 10:46:57   \n",
       "4     https://www.deviantart.com/sandara/art/Riders-...  2013-09-30 23:37:02   \n",
       "...                                                 ...                  ...   \n",
       "1183  https://www.deviantart.com/meganjoy/art/Treasu...  2012-04-02 04:34:29   \n",
       "1184  https://www.deviantart.com/artozi/art/Apotlas-...  2014-10-09 04:35:37   \n",
       "1185  https://www.deviantart.com/nuriko-kun/art/Spir...  2011-04-01 10:44:45   \n",
       "1186  https://www.deviantart.com/roxy317/art/Stamp-I...  2007-11-11 05:17:17   \n",
       "1187  https://www.deviantart.com/yuumei/art/A-Moment...  2017-06-25 12:01:18   \n",
       "\n",
       "      Height  Width  File Size  Author Watchers  Author Page Views  \\\n",
       "0        599    900     408379            36527            1354598   \n",
       "1        800    800     523370            45123            7500677   \n",
       "2        400    400    4493241            35586            1438276   \n",
       "3        532   1000     390864           397714           22504533   \n",
       "4        770   1200     832434           212588            7566524   \n",
       "...      ...    ...        ...              ...                ...   \n",
       "1183     600    600     440725            38660            1861256   \n",
       "1184    1200    900     290057            11424             281437   \n",
       "1185     800   1000     545063            57989            1775664   \n",
       "1186      56     99      18015              227              73692   \n",
       "1187    1326   1500    1225292           397713           22504608   \n",
       "\n",
       "      Author Deviations  \n",
       "0                  2179  \n",
       "1                   358  \n",
       "2                   345  \n",
       "3                   842  \n",
       "4                   796  \n",
       "...                 ...  \n",
       "1183                235  \n",
       "1184                131  \n",
       "1185                188  \n",
       "1186                 32  \n",
       "1187                842  \n",
       "\n",
       "[1188 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85210b9a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#https://github.com/pharmapsychotic/clip-interrogator\\n\\nimport os\\nimport torch\\nfrom PIL import Image\\nfrom clip_interrogator import Config, Interrogator\\n\\n#setting up dataframe for captions.\\ncsv_file = \\'caption_info.csv\\'\\ntry:\\n    caption_df = pd.read_csv(csv_file)\\nexcept FileNotFoundError:\\n    caption_df = pd.DataFrame()\\n\\n#setting up interrogator\\nci = Interrogator(Config(clip_model_name=\"RN50/openai\"))\\n\\nsubset_df = deviation_df[0:3]\\n\\nfor deviation in subset_df.values:\\n    dId = deviation[0]\\n    image = Image.open(f\"images/{dId}.png\").convert(\\'RGB\\')\\n    \\n    caption = ci.interrogate(image)\\n    print(dId)\\n    print(caption)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative clip interrogator.\n",
    "'''\n",
    "#https://github.com/pharmapsychotic/clip-interrogator\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "#setting up dataframe for captions.\n",
    "csv_file = 'caption_info.csv'\n",
    "try:\n",
    "    caption_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    caption_df = pd.DataFrame()\n",
    "\n",
    "#setting up interrogator\n",
    "ci = Interrogator(Config(clip_model_name=\"RN50/openai\"))\n",
    "\n",
    "subset_df = deviation_df[0:3]\n",
    "\n",
    "for deviation in subset_df.values:\n",
    "    dId = deviation[0]\n",
    "    image = Image.open(f\"images/{dId}.png\").convert('RGB')\n",
    "    \n",
    "    caption = ci.interrogate(image)\n",
    "    print(dId)\n",
    "    print(caption)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461f4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b952971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress report: 1188\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#setting up dataframe for captions.\n",
    "csv_file = 'caption_info.csv'\n",
    "try:\n",
    "    caption_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    caption_df = pd.DataFrame()\n",
    "\n",
    "#this is so we can start farther down if we get an error.\n",
    "j = 700\n",
    "subset_df = deviation_df[j:]\n",
    "#save every n captions\n",
    "n = 10\n",
    "\n",
    "i = 0\n",
    "for deviation in subset_df.values:\n",
    "    dId = deviation[0]\n",
    "    image = Image.open(f\"images/{dId}.png\")\n",
    "    \n",
    "    #https://www.reddit.com/r/StableDiffusion/comments/11f938k/using_automatic1111_apis_for_clip/\n",
    "    #https://stackoverflow.com/questions/52494592/wrong-colours-with-cv2-imdecode-python-opencv\n",
    "    #below converts image into string to pass through API\n",
    "    #--------------------------------------------------------------\n",
    "    cv2_image = np.array(image)\n",
    "    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    _, buffer = cv2.imencode('.png', cv2_image)\n",
    "    input_image = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    url = \"http://127.0.0.1:7860/sdapi/v1/interrogate\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    payload = {\n",
    "        \"image\": input_image,\n",
    "        \"model\": \"clip\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        caption = response.json()['caption']\n",
    "    else:\n",
    "        caption = \"NA\"\n",
    "    #----------------------------------------------------------- after this, caption is the caption.\n",
    "    \n",
    "    #add info to caption dataframe.\n",
    "    row = {\n",
    "        'Deviation ID': dId,\n",
    "        'Caption': caption\n",
    "    }\n",
    "    row_df = pd.DataFrame(row, index=[0])\n",
    "    caption_df = pd.concat([caption_df, row_df], ignore_index=True)\n",
    "    \n",
    "    i+=1\n",
    "    if (i > n):\n",
    "        #Just write it out in case of crash, since it'll take a long time.\n",
    "        caption_df.to_csv('caption_info.csv', index=False)\n",
    "        i=0\n",
    "    j+=1\n",
    "    print(f\"Progress report: {j}\", end='\\r')\n",
    "\n",
    "#then save at the end regardless\n",
    "caption_df = caption_df.drop_duplicates(subset='Deviation ID')\n",
    "caption_df.to_csv('caption_info.csv', index=False)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bc734",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133e3eb",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810eb7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 14)\n",
      "(1188, 14)\n",
      "(1188, 2)\n",
      "(1188, 2)\n"
     ]
    }
   ],
   "source": [
    "# read in the csv files which should have been created from the data gathering.\n",
    "deviation_df = pd.read_csv('deviation_info.csv')\n",
    "caption_df = pd.read_csv('caption_info.csv')\n",
    "\n",
    "#drop NA values if they exist in our deviation_df. Fortunately, all of our images have good values.\n",
    "print(deviation_df.shape)\n",
    "deviation_df = deviation_df.dropna()\n",
    "print(deviation_df.shape)\n",
    "\n",
    "#When a caption is failed to be read, it is given \"NA\" for a caption. Remove these from the list.\n",
    "#also if there is a na then we drop it.\n",
    "print(caption_df.shape)\n",
    "caption_df = caption_df[caption_df['Caption'] != \"NA\"]\n",
    "caption_df = caption_df.dropna()\n",
    "print(caption_df.shape)\n",
    "\n",
    "#Drop duplicates of both, just in case, although in our setup we already do this.\n",
    "deviation_df = deviation_df.drop_duplicates(subset='Deviation ID')\n",
    "caption_df = caption_df.drop_duplicates(subset='Deviation ID')\n",
    "\n",
    "#For the most part, our data is already clean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
