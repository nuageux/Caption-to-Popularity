{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project (change this to your project's title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [X] YES - make available\n",
    "* [ ] NO - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Mariam Bachar (A16217374)\n",
    "- Alexandra Hernandez (A16730685)\n",
    "- Brian Kwon (A16306826)\n",
    "- Andrew Uhm (A16729684)\n",
    "- Ethan Wang (A17229824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do certain keywords as identified by CLIP correlate with the popularity (as measured by the equivalent of “likes”) that artwork receives on social media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='background'></a>\n",
    "\n",
    "## Background & Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rise of social media in the past decade, Internet users are constantly exposed to different types of media, especially digital art. To interact with them, users can  “like” and “comment” on another’s content, which also notifies the owner of their received feedback. In turn, the algorithm not only encourages users to continue seeing content from the original user, but also similar “liked” content from different users (Lua, 2023). As a result, social media algorithms–despite having the same code throughout–are customized to each user, showing them content that is relevant to them, based on their past interactions with other accounts and users. Furthermore, these processes incorporate machine learning algorithms, such as feature detection and sentiment classification to better pinpoint content and users that an individual will most likely interact with, boosting engagement for both the receiving user–and the social media itself (T.K. et al., 2021).\n",
    "\n",
    "As social media continues to grow, there continues to be several cases done on the usage of social media algorithms and their role in user engagement. Especially with the introduction of new feature detection technologies–and Artificial Intelligence (AI)–the study field has developed exponentially. In Radion Purba et. al. 's “Instagram Post Popularity Trend Analysis and Prediction” (2020) they looked into social media popularity predictions for marketing purposes; particularly, their research methodology discussed provided a similar workflow to our posed research question. They also discussed definitions of popularity, derived from the data collected. However, the main difference is the study done using hashtags (Radion Purba et al., 2020); we would incorporate AI-generated descriptions of relevant artwork. Similarly, Kafritsas's “CLIP: The Most Influential AI Model from OpenAI and How To Use It” (2022)  details the AI software we plan to use for our data generation. It involves the  AI, CLIP, showcasing  examples of its function, and describes common use cases. CLIP is not necessarily as accurate as other supervised learning models and lacks some context interpreting ability. As we plan to use similar AI technologies in our research, Kafritsas’s usage of CLIP primes us for its use in a concise manner while also precautions us on making certain assumptions when making our analyses.\n",
    "\n",
    "**References**\n",
    "1. Lua, A. (2023, April 20). How the Instagram Algorithm Works in 2023: Everything You Need to Know. Buffer Library; Buffer Library. https://buffer.com/library/instagram-feed-algorithm/#how-does-the-instagram-algorithm-work-the-6-key-ranking-factors\n",
    "\n",
    "2. T.K., B., Annavarapu, C. S. R., & Bablani, A. (2021). Machine learning algorithms for social media analysis: A survey. Computer Science Review, 100395. https://doi.org/10.1016/j.cosrev.2021.100395\n",
    "\n",
    "3. Radion Purba, K., Asirvatham, D., & Kumar Murugesan, R. (2020). Instagram Post Popularity Trend Analysis and Prediction using Hashtag, Image Assessment, and User History Features. The International Arab Journal of Information Technology, 1. https://doi.org/10.34028/iajit/18/1/10\n",
    "\n",
    "4. Kafritsas, N. (2022). CLIP: The Most Influential AI Model From OpenAI — And How To Use It. Medium; Towards Data Science. https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict that digital artwork that contains certain keywords as predicted by CLIP (painting vs. watercolor vs. digital) will indeed have a positive correlation to popularity on social media. As humans observing what is popular, we notice that certain features tend to repeat themselves across posts, which leads us to believe a correlation will be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset Name: deviation_info\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/blob/master/deviation_info.csv\n",
    "- Number of observations: 1188\n",
    "\n",
    "This dataset is a set of deviations (that is images from deviantart) that contain deviation ids and metadata about the deviation itself as well as the author. It does not include the actual images.\n",
    "\n",
    "- Dataset Name: caption_info\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/blob/master/caption_info.csv\n",
    "- Number of observations: 1188\n",
    "\n",
    "This dataset is a set of captions processed from the image, corresponding to a deviation id. It was processed using the CLIP interrogator in Automatic1111's stable diffusion webui.\n",
    "\n",
    "- Dataset Name: images\n",
    "- Link to the dataset: https://github.com/COGS108/Group_Sp23_Project_Group_3/tree/master/images\n",
    "- Number of observations: 1198\n",
    "\n",
    "This dataset is a directory of images in png format that are named based on their corresponding deviation ids, it contains the actual images. There are 10 extra images in here that aren't found in our other datasets.\n",
    "\n",
    "All of the datasets were built from scraping, and use deviation ids as their identifiers. Because of this, we can easily add them together based on those deviation ids if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_252/1826982723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import deviantart\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeviantArt API: https://www.deviantart.com/developers/http/v1/20210526\n",
    "# Open-Source Python wrapper for DA API: https://github.com/neighbordog/deviantart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a pd df from the CSV file if it exists, else creates a blank df.\n",
    "csv_file = 'deviation_info.csv'\n",
    "try:\n",
    "    deviation_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    deviation_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# README: use your own token\n",
    "cur_access = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# how many images we want to fetch * 10\n",
    "n = 120"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(n):\n",
    "    print('on iteration', i, '* 10')\n",
    "    # grab 10 images at a time. DeviantArt calls their posts \"deviations\".\n",
    "    deviations = cur_access.browse(endpoint='popular', timerange='alltime', offset=i*10, limit=10)['results']\n",
    "    \n",
    "    for deviation in deviations:\n",
    "        # saves image to file by deviation id using url for local CLIP analysis\n",
    "        if deviation.content is None:\n",
    "            print('null deviation on iteration', i)\n",
    "            continue\n",
    "        url = deviation.content['src']\n",
    "        dId = deviation.deviationid\n",
    "        filename = f\"images/{dId}.png\"\n",
    "        path = Path(filename)\n",
    "        if path.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            open(filename, 'w').close()\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "        # these serve as examples of how to make a request when the python wrapper doesn't work\n",
    "        username = deviation.author.username\n",
    "        request = f\"https://www.deviantart.com/api/v1/oauth2/user/profile/{username}?access_token={cur_access.access_token}&expand=user.stats\"\n",
    "        response = requests.get(request)\n",
    "        authorData = response.json()\n",
    "        authorWatchers = authorData['user']['stats']['watchers']\n",
    "        authorPageViews = authorData['stats']['profile_pageviews'] # deemed unnecessary?\n",
    "        authorDeviations = authorData['stats']['user_deviations']\n",
    "        \n",
    "        request = f\"https://www.deviantart.com/api/v1/oauth2/deviation/metadata?access_token={cur_access.access_token}&deviationids={deviation}&ext_stats=True\"\n",
    "        response = requests.get(request)\n",
    "        metaData = response.json()\n",
    "        views = metaData['metadata'][0]['stats']['views']\n",
    "        \n",
    "        # gathering relevant data, turning it into a new observation.\n",
    "        row = {\n",
    "            'Deviation ID': deviation.deviationid,\n",
    "            'Title': deviation.title,\n",
    "            'Author': deviation.author,\n",
    "            'Views': views,\n",
    "            'Favorites': deviation.stats['favourites'],\n",
    "            'Comments': deviation.stats['comments'],\n",
    "            'URL Link': deviation.url,\n",
    "            'Date Posted': datetime.fromtimestamp(int(deviation.published_time)),\n",
    "            'Height': deviation.content['height'],\n",
    "            'Width': deviation.content['width'],\n",
    "            'File Size': deviation.content['filesize'],\n",
    "            'Author Watchers': authorWatchers,\n",
    "            'Author Page Views': authorPageViews,\n",
    "            'Author Deviations': authorDeviations\n",
    "        }\n",
    "        row_df = pd.DataFrame(row, index=[0])\n",
    "        deviation_df = pd.concat([deviation_df, row_df], ignore_index=True)\n",
    "        \n",
    "    # when running on the most popular posts, we will likely get duplicates. remove them.\n",
    "    deviation_df = deviation_df.drop_duplicates(subset='Deviation ID')\n",
    "    \n",
    "    # grab every 15 seconds in order to adhere to DeviantArt fetch rate.\n",
    "    if n > 1:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# put our dataframe into a CSV file so scraping can be collaborative.\n",
    "# to_csv overwrites but should be OK since we are reading from the CSV to populate the dataframe anyways.\n",
    "deviation_df.to_csv('deviation_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# setting up dataframe for captions.\n",
    "csv_file = 'caption_info.csv'\n",
    "try:\n",
    "    caption_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    caption_df = pd.DataFrame()\n",
    "\n",
    "# this is so we can start farther down if we get an error.\n",
    "j = 700\n",
    "subset_df = deviation_df[j:]\n",
    "# save every n captions.\n",
    "n = 10\n",
    "\n",
    "i = 0\n",
    "for deviation in subset_df.values:\n",
    "    dId = deviation[0]\n",
    "    image = Image.open(f\"images/{dId}.png\")\n",
    "    \n",
    "    # https://www.reddit.com/r/StableDiffusion/comments/11f938k/using_automatic1111_apis_for_clip/\n",
    "    # https://stackoverflow.com/questions/52494592/wrong-colours-with-cv2-imdecode-python-opencv\n",
    "    # below converts image into string to pass through API.\n",
    "    # --------------------------------------------------------------\n",
    "    cv2_image = np.array(image)\n",
    "    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    _, buffer = cv2.imencode('.png', cv2_image)\n",
    "    input_image = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    url = \"http://127.0.0.1:7860/sdapi/v1/interrogate\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    payload = {\n",
    "        \"image\": input_image,\n",
    "        \"model\": \"clip\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        caption = response.json()['caption']\n",
    "    else:\n",
    "        caption = \"NA\"\n",
    "    # ----------------------------------------------------------- after this, caption is the caption.\n",
    "    \n",
    "    # add info to caption dataframe.\n",
    "    row = {\n",
    "        'Deviation ID': dId,\n",
    "        'Caption': caption\n",
    "    }\n",
    "    row_df = pd.DataFrame(row, index=[0])\n",
    "    caption_df = pd.concat([caption_df, row_df], ignore_index=True)\n",
    "    \n",
    "    i+=1\n",
    "    if (i > n):\n",
    "        # write it out in case of crash, since it'll take a long time.\n",
    "        caption_df.to_csv('caption_info.csv', index=False)\n",
    "        i=0\n",
    "    j+=1\n",
    "    print(f\"Progress report: {j}\", end='\\r')\n",
    "\n",
    "# then save at the end regardless.\n",
    "caption_df = caption_df.drop_duplicates(subset='Deviation ID')\n",
    "caption_df.to_csv('caption_info.csv', index=False)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "caption_df = pd.read_csv('caption_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll do some basic cleaning where we drop N/A values and duplicates."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# read in the .csv files which should have been created from the data gathering.\n",
    "deviation_df = pd.read_csv('deviation_info.csv')\n",
    "caption_df = pd.read_csv('caption_info.csv')\n",
    "\n",
    "# drop NA values if they exist in our deviation_df. Fortunately, all of our images have good values.\n",
    "print(deviation_df.shape)\n",
    "deviation_df = deviation_df.dropna()\n",
    "print(deviation_df.shape)\n",
    "\n",
    "# when a caption is failed to be read, it is given \"NA\" for a caption. Remove these from the list.\n",
    "# also if there is an NA then we drop it.\n",
    "print(caption_df.shape)\n",
    "caption_df = caption_df[caption_df['Caption'] != \"NA\"]\n",
    "caption_df = caption_df.dropna()\n",
    "print(caption_df.shape)\n",
    "\n",
    "# drop duplicates of both, just in case, although in our setup we already do this.\n",
    "deviation_df = deviation_df.drop_duplicates(subset='Deviation ID')\n",
    "caption_df = caption_df.drop_duplicates(subset='Deviation ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if new session, retrieve our deviation info\n",
    "csv_file = 'deviation_info.csv'\n",
    "try:\n",
    "    deviation_df = pd.read_csv(csv_file)\n",
    "except:\n",
    "    FileNotFoundError\n",
    "    \n",
    "# if new session, retrieve our CLIP caption info\n",
    "csv_file = 'caption_info.csv'\n",
    "try:\n",
    "    caption_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    caption_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join our dataframes together based on their common ID for easier usage later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deviation ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Views</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Comments</th>\n",
       "      <th>URL Link</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>File Size</th>\n",
       "      <th>Author Watchers</th>\n",
       "      <th>Author Page Views</th>\n",
       "      <th>Author Deviations</th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6797CD44-47EA-B405-9377-5D41D83F33FE</td>\n",
       "      <td>A l'aise...</td>\n",
       "      <td>thrumyeye</td>\n",
       "      <td>2022815</td>\n",
       "      <td>31101</td>\n",
       "      <td>2385</td>\n",
       "      <td>https://www.deviantart.com/thrumyeye/art/A-l-a...</td>\n",
       "      <td>2011-02-17 23:43:04</td>\n",
       "      <td>599</td>\n",
       "      <td>900</td>\n",
       "      <td>408379</td>\n",
       "      <td>36527</td>\n",
       "      <td>1354598</td>\n",
       "      <td>2179</td>\n",
       "      <td>a red fox is sitting in the grass and looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83750DFB-D23E-00A3-DF4F-D164A07DF605</td>\n",
       "      <td>Tiger cub</td>\n",
       "      <td>Kamirah</td>\n",
       "      <td>1271452</td>\n",
       "      <td>20720</td>\n",
       "      <td>2097</td>\n",
       "      <td>https://www.deviantart.com/kamirah/art/Tiger-c...</td>\n",
       "      <td>2008-07-11 06:10:53</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "      <td>523370</td>\n",
       "      <td>45123</td>\n",
       "      <td>7500677</td>\n",
       "      <td>358</td>\n",
       "      <td>a tiger sitting on top of a rock in a forest w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8F1ED8A1-88A5-861A-F83B-77916A1481A0</td>\n",
       "      <td>Baby Steps 0268P</td>\n",
       "      <td>Sooper-Deviant</td>\n",
       "      <td>1097719</td>\n",
       "      <td>24050</td>\n",
       "      <td>937</td>\n",
       "      <td>https://www.deviantart.com/sooper-deviant/art/...</td>\n",
       "      <td>2009-10-05 06:55:37</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>4493241</td>\n",
       "      <td>35586</td>\n",
       "      <td>1438276</td>\n",
       "      <td>345</td>\n",
       "      <td>a red panda cub walking on a tree branch in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66D5BA39-C0D4-7A95-52FF-C0694149142E</td>\n",
       "      <td>Sky Turtle</td>\n",
       "      <td>yuumei</td>\n",
       "      <td>1056323</td>\n",
       "      <td>23103</td>\n",
       "      <td>918</td>\n",
       "      <td>https://www.deviantart.com/yuumei/art/Sky-Turt...</td>\n",
       "      <td>2014-01-23 10:46:57</td>\n",
       "      <td>532</td>\n",
       "      <td>1000</td>\n",
       "      <td>390864</td>\n",
       "      <td>397714</td>\n",
       "      <td>22504533</td>\n",
       "      <td>842</td>\n",
       "      <td>a bird flying through the air with its wings s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1C127EBF-EFF7-7BC4-004D-0355A2856C05</td>\n",
       "      <td>Riders</td>\n",
       "      <td>sandara</td>\n",
       "      <td>1240225</td>\n",
       "      <td>15643</td>\n",
       "      <td>512</td>\n",
       "      <td>https://www.deviantart.com/sandara/art/Riders-...</td>\n",
       "      <td>2013-09-30 23:37:02</td>\n",
       "      <td>770</td>\n",
       "      <td>1200</td>\n",
       "      <td>832434</td>\n",
       "      <td>212588</td>\n",
       "      <td>7566524</td>\n",
       "      <td>796</td>\n",
       "      <td>a man riding on the back of a horse next to a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Deviation ID             Title          Author  \\\n",
       "0  6797CD44-47EA-B405-9377-5D41D83F33FE       A l'aise...       thrumyeye   \n",
       "1  83750DFB-D23E-00A3-DF4F-D164A07DF605         Tiger cub         Kamirah   \n",
       "2  8F1ED8A1-88A5-861A-F83B-77916A1481A0  Baby Steps 0268P  Sooper-Deviant   \n",
       "3  66D5BA39-C0D4-7A95-52FF-C0694149142E        Sky Turtle          yuumei   \n",
       "4  1C127EBF-EFF7-7BC4-004D-0355A2856C05            Riders         sandara   \n",
       "\n",
       "     Views  Favorites  Comments  \\\n",
       "0  2022815      31101      2385   \n",
       "1  1271452      20720      2097   \n",
       "2  1097719      24050       937   \n",
       "3  1056323      23103       918   \n",
       "4  1240225      15643       512   \n",
       "\n",
       "                                            URL Link          Date Posted  \\\n",
       "0  https://www.deviantart.com/thrumyeye/art/A-l-a...  2011-02-17 23:43:04   \n",
       "1  https://www.deviantart.com/kamirah/art/Tiger-c...  2008-07-11 06:10:53   \n",
       "2  https://www.deviantart.com/sooper-deviant/art/...  2009-10-05 06:55:37   \n",
       "3  https://www.deviantart.com/yuumei/art/Sky-Turt...  2014-01-23 10:46:57   \n",
       "4  https://www.deviantart.com/sandara/art/Riders-...  2013-09-30 23:37:02   \n",
       "\n",
       "   Height  Width  File Size  Author Watchers  Author Page Views  \\\n",
       "0     599    900     408379            36527            1354598   \n",
       "1     800    800     523370            45123            7500677   \n",
       "2     400    400    4493241            35586            1438276   \n",
       "3     532   1000     390864           397714           22504533   \n",
       "4     770   1200     832434           212588            7566524   \n",
       "\n",
       "   Author Deviations                                            Caption  \n",
       "0               2179  a red fox is sitting in the grass and looking ...  \n",
       "1                358  a tiger sitting on top of a rock in a forest w...  \n",
       "2                345  a red panda cub walking on a tree branch in th...  \n",
       "3                842  a bird flying through the air with its wings s...  \n",
       "4                796  a man riding on the back of a horse next to a ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joining the two dataframes for good measure.\n",
    "df = pd.merge(deviation_df, caption_df, on='Deviation ID', how='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to anonymize the data to maintain ethical integrity as discussed in our project proposal. Let's remove the Author field, as it doesn't contribute anything otherwise. We don't need the URL anymore and it also reveals the author on top of that, so we remove it too. We are essentially using the caption in place of the title, so we will disregard it. We also acknowledge that webpage \"views\" can be manipulated by bots. The views on the deviation (read: artwork) are still relevant despite knowing that, but the views on the Author's page matter less so. Let's remove that also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('URL Link', axis=1)\n",
    "df = df.drop('Author', axis=1)\n",
    "df = df.drop('Author Page Views', axis=1)\n",
    "df = df.drop('Title', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deviation ID</th>\n",
       "      <th>Views</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>File Size</th>\n",
       "      <th>Author Watchers</th>\n",
       "      <th>Author Deviations</th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6797CD44-47EA-B405-9377-5D41D83F33FE</td>\n",
       "      <td>2022815</td>\n",
       "      <td>31101</td>\n",
       "      <td>2385</td>\n",
       "      <td>2011-02-17 23:43:04</td>\n",
       "      <td>599</td>\n",
       "      <td>900</td>\n",
       "      <td>408379</td>\n",
       "      <td>36527</td>\n",
       "      <td>2179</td>\n",
       "      <td>a red fox is sitting in the grass and looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83750DFB-D23E-00A3-DF4F-D164A07DF605</td>\n",
       "      <td>1271452</td>\n",
       "      <td>20720</td>\n",
       "      <td>2097</td>\n",
       "      <td>2008-07-11 06:10:53</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "      <td>523370</td>\n",
       "      <td>45123</td>\n",
       "      <td>358</td>\n",
       "      <td>a tiger sitting on top of a rock in a forest w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8F1ED8A1-88A5-861A-F83B-77916A1481A0</td>\n",
       "      <td>1097719</td>\n",
       "      <td>24050</td>\n",
       "      <td>937</td>\n",
       "      <td>2009-10-05 06:55:37</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>4493241</td>\n",
       "      <td>35586</td>\n",
       "      <td>345</td>\n",
       "      <td>a red panda cub walking on a tree branch in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66D5BA39-C0D4-7A95-52FF-C0694149142E</td>\n",
       "      <td>1056323</td>\n",
       "      <td>23103</td>\n",
       "      <td>918</td>\n",
       "      <td>2014-01-23 10:46:57</td>\n",
       "      <td>532</td>\n",
       "      <td>1000</td>\n",
       "      <td>390864</td>\n",
       "      <td>397714</td>\n",
       "      <td>842</td>\n",
       "      <td>a bird flying through the air with its wings s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1C127EBF-EFF7-7BC4-004D-0355A2856C05</td>\n",
       "      <td>1240225</td>\n",
       "      <td>15643</td>\n",
       "      <td>512</td>\n",
       "      <td>2013-09-30 23:37:02</td>\n",
       "      <td>770</td>\n",
       "      <td>1200</td>\n",
       "      <td>832434</td>\n",
       "      <td>212588</td>\n",
       "      <td>796</td>\n",
       "      <td>a man riding on the back of a horse next to a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Deviation ID    Views  Favorites  Comments  \\\n",
       "0  6797CD44-47EA-B405-9377-5D41D83F33FE  2022815      31101      2385   \n",
       "1  83750DFB-D23E-00A3-DF4F-D164A07DF605  1271452      20720      2097   \n",
       "2  8F1ED8A1-88A5-861A-F83B-77916A1481A0  1097719      24050       937   \n",
       "3  66D5BA39-C0D4-7A95-52FF-C0694149142E  1056323      23103       918   \n",
       "4  1C127EBF-EFF7-7BC4-004D-0355A2856C05  1240225      15643       512   \n",
       "\n",
       "           Date Posted  Height  Width  File Size  Author Watchers  \\\n",
       "0  2011-02-17 23:43:04     599    900     408379            36527   \n",
       "1  2008-07-11 06:10:53     800    800     523370            45123   \n",
       "2  2009-10-05 06:55:37     400    400    4493241            35586   \n",
       "3  2014-01-23 10:46:57     532   1000     390864           397714   \n",
       "4  2013-09-30 23:37:02     770   1200     832434           212588   \n",
       "\n",
       "   Author Deviations                                            Caption  \n",
       "0               2179  a red fox is sitting in the grass and looking ...  \n",
       "1                358  a tiger sitting on top of a rock in a forest w...  \n",
       "2                345  a red panda cub walking on a tree branch in th...  \n",
       "3                842  a bird flying through the air with its wings s...  \n",
       "4                796  a man riding on the back of a horse next to a ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can justify the rest of the columns. We need the ID for identification purposes. The title may relate to the CLIP caption. The views, favorites, and comments are how we are gauging the popularity of the post. The date posted is relevant for trend analysis. The file size, height, and width implies the rendering definition (e.g. 1080px) of the work. Author watchers and deviations may imply how much experience or traction this author has had on DeviantArt. Finally, the CLIP caption is integral to our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider adjusting the string-based content of our data. Below we notice that the AI caption is longer than we need it to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a red fox is sitting in the grass and looking at the camera with a sad look on its face, Arie Smit, animal photography, a stock photo, furry art\n",
      "\n",
      "a drawing of a bunch of arrows with different designs on them and numbers on them, all of which are different, Évariste Vital Luminais, tarot card, concept art, symbolism\n",
      "\n",
      "a painting of a winged creature standing in a forest next to a lake with a waterfall in the background, Bastien Lecouffe-Deharme, magic the gathering artwork, concept art, fantasy art\n",
      "\n",
      "a drawing of a umbrella and a clock on a book page with a pen and ink drawing of a woman holding an umbrella, Farel Dalrymple, illustrated, a storybook illustration, pop surrealism\n",
      "\n",
      "a drawing of a woman with blue hair and a blue eyeliner on a notebook with a pen and a pen, Android Jones, trending on art station, a watercolor painting, process art\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, 1000, 200):\n",
    "    print(df['Caption'][n])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that mainly what is before the first comma is the only relevant part of the caption. Furthermore, the AI makes a guess at who made the picture after the first comma. Let's remove everything after the first comma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(df['Caption'].size):\n",
    "    before_comma = df['Caption'][n].split(\",\")[0]\n",
    "    df.loc[n, 'Caption'] = before_comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a forest with a lot of trees and snow on the ground and sun shining through the trees and the ground'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Caption'][30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the caption is much more precise. However, we do not want the stopwords such as \"a\", \"is\", \"the\", etc. within our analysis, as they are irrelevant. Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_252/2693805424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(df['Caption'].size):\n",
    "    tokens = word_tokenize(df['Caption'][n])\n",
    "    filtered_text = [word for word in tokens if word not in stopwords]\n",
    "    if \"'s\" in filtered_text:\n",
    "        filtered_text.remove(\"'s\")\n",
    "    filtered_text = list(set(filtered_text))\n",
    "    filtered_caption = ' '.join(filtered_text)\n",
    "    df.loc[n, 'Caption'] = filtered_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Caption'][30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of our Data Analysis would be to determine the popularity metric by using views, favorites, and comments fields. This way, we can dive deeper into our analysis and explore 3 main popularity trends: fileSizeAndQuality, authorExperience, and the AICaption.\n",
    "\n",
    "Next, we will run a statistical test to see whether popularity IS NOT related to fileSizeAndQuality or authorExperience, and if popularity IS related to the AICaption. It's not critical that these statistical tests pass, rather we just need the results to evaluate whether our hypothesis is true or false;\n",
    "\n",
    "We predict that digital artwork that contains certain keywords as predicted by CLIP (painting vs. watercolor vs. digital) will indeed have a positive correlation to popularity on social media. As humans observing what is popular, we notice that certain features tend to repeat themselves across posts, which leads us to believe a correlation will be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define a popularity metric in order to do meaningful analysis. We explore the views, favorites, comments, and author watcher fields to get an idea of what a reasonable metric to measure popularity would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Views']/100)\n",
    "plt.xlim(0, 40000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shown above is a histogram of the artwork views divided by 100. The x-axis represents the range of views, while the y-axis represents the frequency or count of artworks falling within each range. The majority of artworks fall between 0 and 5000 views.\n",
    "\n",
    "In terms of the research question and hypothesis, the histogram provides an overview of the distribution of artwork views on social media. By examining the histogram, we can analyze the relationship between the popularity of artwork (measured by views) and the keywords present in the artwork captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Favorites'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shown is a histogram of the number of favorites (equivalent to \"likes\") received by the artworks. The histogram allows us to examine the relationship between the popularity of the artworks (measured by the number of favorites) and the keywords present in their captions. The x-axis represents the range of favorites, while the y-axis represents the frequency or count of artworks falling within each range. As one can see, majority of art received favorites between 5000 to 15000.\n",
    "\n",
    "By looking at the histogram, we can gain insights into how artworks are being received by the audience in terms of favorites. The distribution of favorites can indicate whether certain artworks are more popular or resonate better with the viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Comments'])\n",
    "plt.xlim(0, 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shown is a histogram of the number of comments received by the artworks. Similar to the previous plot, the x-axis represents the range of comments, while the y-axis represents the frequency or count of artworks falling within each range, with the majority falling between 0 and 1000 comments.\n",
    "\n",
    "Analyzing this plot in relation to the research question and hypothesis, it provides an overview of the distribution of comments received by the artworks on social media. The histogram allows us to examine the relationship between the engagement level of the artworks (measured by the number of comments) and the keywords present in their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the distribution for views, favorites, and comments are all similar but differ obviously on the scale. There are a handful of outliers here and there (two points at around 10 million and 6 million are excluded in the views plot and around 5 points above 10 thousand were excluded in the comments plot), but for the most part most people do not get a lot of interaction on their posts (which is how popularity intuitively works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Author Watchers'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author distribution is certainly not normal but not necessarily as clearly right-skewed as the popularity metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that to put the popularity distributions on a similar scale, we can use the scale factors below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_scale_factor = 0.01\n",
    "favorite_scale_factor = 1.0\n",
    "comment_scale_factor = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can divide by Author Watcher count to normalize popularity based on how large a following an author already has (and thus has contributed to the popularity of the artwork):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_metrics = []\n",
    "for n in range(df.shape[0]):\n",
    "    view_count = df['Views'][n]\n",
    "    favorite_count = df['Favorites'][n]\n",
    "    comment_count = df['Comments'][n]\n",
    "    watcher_count = df['Author Watchers'][n]\n",
    "    popularity_metric = (view_count*view_scale_factor + favorite_count*favorite_scale_factor + comment_count*comment_scale_factor) / watcher_count\n",
    "    popularity_metrics.append(popularity_metric * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_metric_df = pd.DataFrame(popularity_metrics).rename(columns={0: 'Popularity Metric'})\n",
    "popularity_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, popularity_metric_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(popularity_metrics)\n",
    "plt.xlim(0, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made a best attempt at normalizing our popularity data, we want to convert it into rank statistics, as popularity data is inherently ordinal in nature if we're sampling starting from the most popular (not to mention we have some pretty wild outliers). We'll use scipy's rankdata and append that to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip ranks so we get the highest metric = rank 1\n",
    "ranks = len(df) + 1 - rankdata(df['Popularity Metric'])\n",
    "df['Rank'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a curiosity, let's order by ranking and see some of the most popular (as defined by us, of course) works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values('Rank').reset_index(drop=True)\n",
    "sorted_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation with the highest popularity metric\n",
    "most_popular_id = sorted_df.loc[0][0]\n",
    "most_popular_index = sorted_df[sorted_df['Deviation ID'] == f'{most_popular_id}'].index[0]\n",
    "print(sorted_df[sorted_df['Deviation ID'] == f'{most_popular_id}'].loc[most_popular_index][10])\n",
    "display(Image(filename=f'./images/{most_popular_id}.png', width=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation with the fourth highest popularity metric\n",
    "# ...choosing the fourth since second and third place are both also \"stamp\" memes like the most popular\n",
    "second_most_popular_id = sorted_df.loc[3][0]\n",
    "second_most_popular_index = sorted_df[sorted_df['Deviation ID'] == f'{second_most_popular_id}'].index[0]\n",
    "print(sorted_df[sorted_df['Deviation ID'] == f'{second_most_popular_id}'].loc[second_most_popular_index][10])\n",
    "display(Image(filename=f'./images/{second_most_popular_id}.png', width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to observe the data and to see the effects of keywords on popularity, we decided to aggregate all of the popularity data into a single dataframe with only the popularity values paired with keywords. In order to determine the popularity of a keyword, we decided to take the average of the popularities of each keyword and use that value to determine the overall popularity. We also removed all words that appeared less than 3 times in order to remove some inconsistent words that had high popularity rankings that appeared very few times. In order to do this, we used the values from the dictionary unique to create the new dataframe. We also did a log on the data in order to further deal with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's store the keywords and popularity statistics into one dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique is a dict that contains a list of tuples of (Deviation ID, Popularity Metric, Rank) for each unique word in our df.\n",
    "# the frequency of a key (word) is the length.\n",
    "unique = {}\n",
    "for n in range(df.shape[0]):\n",
    "    tokens = word_tokenize(df['Caption'][n])\n",
    "    for word in tokens:\n",
    "        if word in unique:\n",
    "            unique[word].append((df['Deviation ID'][n], df['Popularity Metric'][n], df['Rank'][n]))\n",
    "        else:\n",
    "            # tuple that contains (freq, deviation ID)\n",
    "            unique[word] = [(df['Deviation ID'][n], df['Popularity Metric'][n], df['Rank'][n])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example:\n",
    "unique['branch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further apply a Term Frequency - Inverse Document Frequency (TFIDF) vectorizer on top of this data to better scale keywords to their uniqueness within our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                        analyzer='word', \n",
    "                        tokenizer=word_tokenize, \n",
    "                        stop_words=list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_tokens = tfidf.fit_transform(df['Caption'])\n",
    "vectorized_tokens = pd.DataFrame(vectorized_tokens.toarray(),\n",
    "                                 columns=tfidf.get_feature_names_out())\n",
    "vectorized_tokens['Deviation ID'] = df['Deviation ID']\n",
    "vectorized_tokens = vectorized_tokens.set_index('Deviation ID')\n",
    "\n",
    "words = []\n",
    "pop_val = []\n",
    "sums = 0.0\n",
    "for word in unique:\n",
    "    if(len(unique[word]) > 2):\n",
    "        sums = 0.0\n",
    "        words.append(word)\n",
    "        for values in unique[word]:\n",
    "            sums += values[1] * vectorized_tokens.loc[values[0],word]\n",
    "        pop_val.append(sums/len(unique[word]))\n",
    "keyword_pop = pd.DataFrame(list(zip(words, pop_val)))\n",
    "keyword_pop.columns = ['keyword', 'popularity']\n",
    "keyword_pop = keyword_pop.sort_values('popularity').reset_index(drop=True)\n",
    "\n",
    "sorted_keyword_pop = keyword_pop.sort_values('popularity', ascending=False)\n",
    "sorted_keyword_pop.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "pop_val = []\n",
    "sums = 0.0\n",
    "for word in unique:\n",
    "    words.append(word)\n",
    "    for values in unique[word]:\n",
    "        sums += values[1]\n",
    "    # add only if appears more than 3 times\n",
    "    if (len(unique[word]) > 2):\n",
    "        pop_val.append(sums/(len(unique[word])))\n",
    "    sums = 0.0\n",
    "keyword_pop = pd.DataFrame(list(zip(words, pop_val)))\n",
    "keyword_pop.columns = ['keyword', 'popularity']\n",
    "keyword_pop = keyword_pop.sort_values('popularity').reset_index(drop=True)\n",
    "keyword_pop.loc[keyword_pop['keyword'] == 'branch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally do a normalization involving logarithmic scaling and graph our findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_series = np.log(keyword_pop.popularity)\n",
    "popularity_graph = sns.histplot(log_series)\n",
    "\n",
    "# setting titles for clarification\n",
    "popularity_graph.set_title('Keyword Popularity vs. Keyword Frequency')\n",
    "popularity_graph.set_xlabel('Keyword popularity')\n",
    "popularity_graph.set_ylabel('Keyword frequency')\n",
    "\n",
    "keyword_pop['logval'] = log_series\n",
    "keyword_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, the most \"popular\" keywords (such as \"arms\") appear the **LEAST** in frequency, whereas lower to middle end popular keywords are more frequent in DeviantArt posts. Overall, the graph of popular keywords and their frequency is normally distributed. \n",
    "\n",
    "It should be noted that the aggregation of keyword frequency is made up of multiple keywords with similar popularity ratings, giving the rise of either low, middle, or high frequency. Furthermore, the most popular keywords show up in the least amount of DeviantArt posts, so we can see that keywords may possibly *not* drive a post's popularity overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "With that said, let's see if with a machine learning model whether a predictive model can be generated or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import math\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up vectorizers\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                        analyzer='word', \n",
    "                        tokenizer=word_tokenize, \n",
    "                        stop_words=stopwords.words(\"english\"))\n",
    "\n",
    "count = CountVectorizer(analyzer='word',\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stopwords.words(\"english\"))\n",
    "\n",
    "scrambled_df = df\n",
    "training_df = scrambled_df[:-100]\n",
    "\n",
    "training_tfidf = pd.DataFrame(tfidf.fit_transform(training_df['Caption'].tolist()).toarray(), \n",
    "                             index=training_df['Deviation ID'], \n",
    "                             columns=tfidf.get_feature_names_out())\n",
    "training_count = pd.DataFrame(count.fit_transform(training_df['Caption'].tolist()).toarray(),\n",
    "                             index=training_df['Deviation ID'],\n",
    "                             columns=count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up model, fitting it to our caption_tfidf as input data and the popularity metric as the output.\n",
    "tfidfModel = DecisionTreeRegressor(criterion='poisson')\n",
    "tfidfModel.fit(training_tfidf, training_df['Popularity Metric'])\n",
    "#setting up countModel\n",
    "countModel = DecisionTreeRegressor(criterion='poisson')\n",
    "countModel.fit(training_count, training_df['Popularity Metric'])\n",
    "#setting a linear regression model just to see\n",
    "linearModel = LinearRegression()\n",
    "linearModel.fit(training_tfidf, training_df['Popularity Metric'])\n",
    "#setting up an SGDRegressor better suited for sparse space\n",
    "lassoModel = Lasso()\n",
    "lassoModel.fit(training_tfidf, training_df['Popularity Metric'])\n",
    "\n",
    "#test set is just what we didn't include in the training set, make it a suitable input to model\n",
    "#and then make predictions.\n",
    "test_df = scrambled_df[-100:]\n",
    "#\n",
    "test_tfidf = pd.DataFrame(tfidf.transform(test_df['Caption'].tolist()).toarray(), \n",
    "                          index=test_df['Deviation ID'], \n",
    "                          columns=tfidf.get_feature_names_out())\n",
    "test_count = pd.DataFrame(count.transform(test_df['Caption'].tolist()).toarray(), \n",
    "                          index=test_df['Deviation ID'], \n",
    "                          columns=tfidf.get_feature_names_out())\n",
    "#setting up prediction arrays\n",
    "tfidf_pred = tfidfModel.predict(test_tfidf)\n",
    "count_pred = countModel.predict(test_count)\n",
    "#\n",
    "linearTfidf_pred = linearModel.predict(test_tfidf)\n",
    "#\n",
    "lasso_pred = lassoModel.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can compare if the model actually was able to predict anything.\n",
    "mean = training_df['Popularity Metric'].mean()\n",
    "median = training_df['Popularity Metric'].median()\n",
    "amount = len(test_df)\n",
    "tfidfModel_sum = 0\n",
    "countModel_sum = 0 \n",
    "linearTfidf_sum = 0 \n",
    "lasso_sum = 0 \n",
    "mean_sum = 0\n",
    "median_sum = 0\n",
    "zero_sum = 0\n",
    "for i in range(amount):\n",
    "    #print(\"Expected: \", y_pred[i], \"Actual: \", test_set['Popularity Metric'].iloc[i])\n",
    "    tfidfModel_sum += (tfidf_pred[i] - test_df['Popularity Metric'].iloc[i]) ** 2\n",
    "    countModel_sum += (count_pred[i] - test_df['Popularity Metric'].iloc[i]) ** 2\n",
    "    linearTfidf_sum += (linearTfidf_pred[i] - test_df['Popularity Metric'].iloc[i]) ** 2\n",
    "    lasso_sum += (lasso_pred[i] - test_df['Popularity Metric'].iloc[i]) ** 2\n",
    "    mean_sum += (test_df['Popularity Metric'].iloc[i] - mean) ** 2\n",
    "    median_sum += (test_df['Popularity Metric'].iloc[i] - median) ** 2\n",
    "    zero_sum += (test_df['Popularity Metric'].iloc[i] - 0) ** 2\n",
    "\n",
    "print(\"Root Mean Squared Error:\")\n",
    "print(\"Tree TF-IDF: \", (tfidfModel_sum / amount) ** (1/2))\n",
    "print(\"Tree Count: \", (countModel_sum / amount) ** (1/2))\n",
    "print(\"Linear TF-IDF: \", (linearTfidf_sum / amount) ** (1/2))\n",
    "print(\"Lasso TF-IDF: \", (lasso_sum / amount) ** (1/2))\n",
    "print(\"Always Mean: \", (mean_sum / amount) ** (1/2))\n",
    "print(\"Always Median: \", (median_sum / amount) ** (1/2))\n",
    "print(\"Always Zero: \", (zero_sum / amount) ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried a bunch of different models, of which the decision tree models were the most successful. We tested both a variant with a TFIDF and Count vectorizer, and tried both a linear and lasso stochastic gradient descent model with our TF-IDF vectorized tokens. The most successful model was the decision tree TF-IDF model, at first it wasn't very successful until the loss function was changed from its default squared error to poisson. Upon doing so, the model was able to perform better than a simple always choose mean algorithm, which supports our hypothesis and indicates that the popularity of a post can be predicted by the features ascribed by CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_importances = tfidfModel.feature_importances_\n",
    "count_feature_importances = countModel.feature_importances_\n",
    "\n",
    "feature_names_tfidf = tfidf.get_feature_names_out()\n",
    "feature_names_count = count.get_feature_names_out()\n",
    "\n",
    "tfidf_feature_importances_mapping = dict(zip(feature_names_tfidf, tfidf_feature_importances))\n",
    "count_feature_importances_mapping = dict(zip(feature_names_count, count_feature_importances))\n",
    "\n",
    "sorted_tfidf_importances = sorted(tfidf_feature_importances_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tfidf_importances[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the features and their importance of the most and only successful model. Here we can see that a lot of the outliers are represented as very important within our model. It's likely that the model's success isn't predicated on these high importance outliers, but on some combination (not necessarily all) of the other roughly 1100 features, and that within our test data these terms simply didn't show up to express a worse result due to overfitting. Regardless, while we could potentially create a better performing model, a model that performs at all is indicative of a relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_tfidf.reset_index()\n",
    "X = X.iloc[:, 1:]\n",
    "X = sm.add_constant(X)\n",
    "X, y = X.align(training_df['Popularity Metric'], axis=0)\n",
    "\n",
    "statsmodel = sm.OLS(y, X)\n",
    "result = statsmodel.fit()\n",
    "\n",
    "print(result.f_pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also ran an OLS regression model on my vectorized input and popularity rating output, and received an incredibly small p value for the F-test, which implies that having the independent variables (that is, the vectorized tokens) makes the model perform better than if there were no independent variables, which indicates that the subject as described by CLIP does in fact to some extent determine popularity. This falls in line with the previous machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ethical concerns regarding this research question that we must be mindful of as we analyze data. The most obvious issue is that we are tagging artwork as unpopular by virtue of not identifying said artwork as popular. However, this should not be a strong issue as we are not presenting identifying pieces of information of specific pieces of artwork or individual artists, so it should not be possible to label a specific artwork or artist as “unpopular”.\n",
    "\n",
    "In terms of normalization, a possible solution would be to take a ratio between the number of likes on the artwork and the number of followers that certain artist has in order to take into account the disparity between larger artists and smaller artists in terms of popularity, as more popular artists would get more likes due to a larger audience. Additionally, it is entirely possible that our analysis may exclude cultural influences of minority groups. Since those residing in developed countries have more leisure time/resources (such as drawing software or drawing e-tablets), it is plausible that most digital art posted to social media is likely from developed countries. Thus, the work we analyze may disproportionately represent artwork and cultural trends of majority groups of developed countries while glossing over minority groups, which tend to be similar across developed countries.\n",
    "\n",
    "Finally, because the artworks are on a public forum, they have consented to allowing their art to be analyzed. The Deviantart TOS states that you cannot “reproduce, distribute, publicly display or perform, or prepare derivative works”, which does not include the use of the artworks for an analytic survey. Although there is no clear-cut solution for this, it serves us well to keep this fact in mind when drawing conclusions upon our analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mariam**\n",
    "- contribution\n",
    "\n",
    "**Alexandra** \n",
    "- Prepared/initialized the creation of the notebooks (outside of the template)\n",
    "- Completed the project background for the Project Proposal and its proofreadings\n",
    "- Wrote and edited part of script for project video\n",
    "- Did descriptions for statistic graphs for the EDA\n",
    "\n",
    "**Brian**\n",
    "- Wrote the ethics and privacy portion of the Project Proposal\n",
    "- Contributed to data analysis with keyword popularity statistics\n",
    "- Helped with the video script and video creation\n",
    "- Helped finalize the project document\n",
    "\n",
    "**Andrew**\n",
    "   - Contributed to data extraction and generation\n",
    "   - Main work on data cleaning, making dataset ethically useable and applied natural language processing on AI generated captions\n",
    "   - Worked on construction of popularity metric\n",
    "\n",
    "**Ethan**\n",
    "- contribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
